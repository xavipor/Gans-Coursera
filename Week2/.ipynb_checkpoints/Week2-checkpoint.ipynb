{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############   Vanilla Start  ~#################\n",
    "# To import libraries and function to show images\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
    "\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will use b&w images (1channel) 10 as input noise size and 64 as the base of the hidden dimensions\n",
    "#Philosophy a bit different from thje previous week, it is all much more encapsulated here, better. \n",
    "#We will just work with an instance of the class generator with the generator defined inside. \n",
    "class Generator (nn.Module):\n",
    "    def __init__(self, z_dim = 10, im_chain =1, hidden_dim = 64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.gen(nn.Sequential(self.make_gen_block(z_dim, hidden_dim*4)),\n",
    "                self.make_gen_block(hidden_dim*4,hidden_dim*2,kernel_size=4,stride=1),\n",
    "                self.make_gen_block(hidden_dim*2,hidden_dim),\n",
    "                self.make_gen_block(hidden_dim,im_chan,kernel_size=4,final_layer=True))\n",
    "        \n",
    "    def make_gen_block (self, input_channels, output_channels, kernel_size = 3, stride = 2, final_layer = False):\n",
    "        \n",
    "        if not final_layer:\n",
    "            return nn.Sequential(nn.ConvTranspose2d(input_channels,output_channels,kernel_size,stride),\n",
    "                                nn.BatchNorm2d(output_channels),\n",
    "                                nn.ReLU(inplace=True))\n",
    "        else:\n",
    "            return nn.Sequential(nn.ConvTranspose2d(input_channels,output_channels,kernel_size,stride),\n",
    "                                nn.Tanh())\n",
    "        \n",
    "    def unsqueeze_noise(self,noise):\n",
    "        #transform noise to get a copy with heigh and width =1. The channels are the z_dim to allow it to match winth the first convolution layer\n",
    "        return noise.view(len(noise),self.z_dim,1,1)\n",
    "    \n",
    "    def forward(self, noise):\n",
    "        x = self.unsqueeze_noise(noise)\n",
    "        return self.gen(x)\n",
    "    def get_noise(n_samples, z_dim, device = 'cpu'):\n",
    "        return torch.randn(n_samples, z_dim, device = device)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__()\n",
    "    super(Discriminator,self)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3Env",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
